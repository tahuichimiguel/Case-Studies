---
title: "Fraud Detection: Logit Regression, AdaBoost, & SVM"
author: "Mikhail Lara"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Load Data
```{r}
suppressMessages(options(warn = -1))
suppressMessages(library(data.table))
suppressMessages(library(ggplot2))
suppressMessages(library(gridExtra))
suppressMessages(library(glmnet))
suppressMessages(library(caret))
suppressMessages(library(corrplot))
suppressMessages(library(GGally))
suppressMessages(library(kernlab))
suppressMessages(library(gbm))
suppressMessages(library(ada))
suppressMessages(library(plyr))

path <- '/Users/Mikey/Documents/ML-Case-Studies/Credit Card Fraud Detection'
setwd(path)
DT<- fread('creditcard.csv',sep = ',', colClasses = rep('numeric',31))
```

##Checking & Cleaning

The dataset consists of **284807 transactions** with each observation classified as either legitimate(Class=0) or fraudulent(Class=1). Each observation has **30 quantitative fields** that can be used as predictors. 

It should be noted that the 'Time' field is just the time lapse between each observation and the 1st observation in the dataset. It is essentially a row index and provides no meaningful date-time information so **it should be excluded from any preditive analysis**.

```{r}
summary(DT)
```

The data is extremely unbalanced with only ***492 fraudulent transactions**, corresponding to only 1.727%. This degree of skewness is problematic for any predictive model because the majority Class will dominate the calculations, making it difficult to build an accurate classifier for fraudulent transactions. Data resampling is needed to create sub-training sets with more balanced occurrences of both Class levels.

```{r}
#How Many Ocurrences of Each Class
DT[,.N, by = Class]
```

##Data Visualization of Top Predictors

There is poor linear correlation between the transaction Class and the possible predictors. Of the 29 non-trivial fields, only 11 have a correlation with a magnitude greater than 10% and only 2 of those correlations exceed 30%. For the sake of simplicity, only those predictors with a Class correlation of at least 10% will be used to build a logistic model for the odds that a transaction is fraudulent.

```{r}
#Linear Correlation of Fraud with Predictors
invisible(DT[,Class:= as.numeric(Class)])

M<-cor(DT)
tbl<-data.table(correlation = M[31,sort.list(abs(M[31,1:30]),decreasing = TRUE)],
                varname=names(DT)[sort.list(abs(M[31,1:30]),decreasing = TRUE)])
tbl

g1<-ggplot(data = DT, aes(x = V17,fill = as.factor(Class)))+geom_density(alpha = 0.25)
g2<-ggplot(data = DT, aes(x = V14,fill = as.factor(Class)))+geom_density(alpha = 0.25)
g3<-ggplot(data = DT, aes(x = V12,fill = as.factor(Class)))+geom_density(alpha = 0.25)

grid.arrange(g1,g2,g3,ncol=3)

g4<-ggplot(data = DT, aes(x = V10,fill = as.factor(Class)))+geom_density(alpha = 0.25)
g5<-ggplot(data = DT, aes(x = V16,fill = as.factor(Class)))+geom_density(alpha = 0.25)
g6<-ggplot(data = DT, aes(x = V3,fill = as.factor(Class)))+geom_density(alpha = 0.25)

grid.arrange(g4,g5,g6,ncol=3)

g7<-ggplot(data = DT, aes(x = V7,fill = as.factor(Class)))+geom_density(alpha = 0.25)
g8<-ggplot(data = DT, aes(x = V11,fill = as.factor(Class)))+geom_density(alpha = 0.25)
g9<-ggplot(data = DT, aes(x = V4,fill = as.factor(Class)))+geom_density(alpha = 0.25)

grid.arrange(g7,g8,g9,ncol=3)

g10<-ggplot(data = DT, aes(x = V18,fill = as.factor(Class)))+geom_density(alpha = 0.25)
g11<-ggplot(data = DT, aes(x = V1,fill = as.factor(Class)))+geom_density(alpha = 0.25)

grid.arrange(g10,g11,ncol=2)
```

#Create Data Partition (Train,Test)
There are 2 major issues with directly using the raw data for modeling
  1. The dataset is too large to use in model building all at once. This is true even if a 60/40 split is used.
  2. The dataset is unbalanced and needs to be adjusted so the fraud data is not modeled as just 'noise'.

Both issues are addressed by making several smaller subsets of the training data that are only 42% the size of the original dataset and are rebalanced so that the fraud/legitimate disparity is less severe. Specifically, a **primary training set** is created using 60% of the raw data(i.e. 60% of all fraud & 60% of all legitimate) and **10 sub-training sets ** are created from it via resampling without replacement. The sub-training sets are created in such a way that the frequency of legitimate transactions is only 8 times that of the fraudulent transactions. 

**Note:** The choice of a 8:1 legitimate/fraud ratio is arbitrary. A complete analysis could include assessing the effect of this split on the predictive power to detect fraud.

```{r}
invisible(DT[,Class:= as.factor(Class)])

set.seed(1234)
inTrain<-createDataPartition(DT$Class, p = 0.6,list=FALSE)
train <-DT[inTrain]  #419 Total Occurrences of Fraud
test  <-DT[-inTrain] #73 Occurrences of Fraud

nonfraud<-which(train$Class==0)
fraud<-which(train$Class==1)

#Use nonfraud-to-fraud ratio of 8:1 
set.seed(1234)

train.sub<-list()
for(i in 1:10){
  train.sub[[i]]<-train[c(sample(fraud,length(fraud)*(.75)),sample(nonfraud,length(fraud)*8*(.75)))]
}
```

#Traditional Logistic Regression

Regularized regression approach would normally be used to handle correlation between the predictors to create an accurate linear model with the fewest number of correlated predictors. However, the values provided in the dataset have been 'decorrelated' via principle component analysis(PCA) resulting is essentially independent predictors. Therefore, standard logistic regression considering only major effects is used.

```{r}
logit.models<-list()

set.seed(1234)
for(i in 1:10){
        logit.models[[i]]<- glm(Class~V17+V14+V12+V10+V16+V3+V7+V11+ V4+ V18+ V1 ,
                                    data = train.sub[[i]],family = 'binomial')
}
```

The predictors used for the final logistic model are selected based on the p-values for each term in the models built on the 10 sub-training datasets. A **p-value of 0.01** is used as the significance threshold for terms in the glm.

```{r}
for(i in 1:10){
 print(t(which(coef(summary(logit.models[[i]]))[,4]<.01)))
}

set.seed(1234)
logistic.final<-glm(data= train,as.factor(Class)~V14+V10+V16+V3+V4,family = 'binomial')
summary(logistic.final)
```

###Logistic Model Accuracy

The standard confusion matrix metrics are calculated using both the **primary training set** and the **test set**
    1. Recall
    2. True Negative Rate (TNR)
    3. Positive Predictive Value (PPV)
    4. Negative Predictive Value (NPV)
    
```{r}
#In-Sample Prediction
  pred.logit<-predict.glm(logistic.final,newdata=train)
  pred.expit<- exp(pred.logit)/(1+exp(pred.logit))
  
  a<-which(train$Class==1)
  b<-which(train$Class==0)
  
  TP<-length(which(pred.expit[a]>0.5))
  FN<-length(which(pred.expit[a]<=0.5))
  TN<-length(which(pred.expit[b]<0.5))
  FP<-length(which(pred.expit[b]>=0.5))
  Recall<-TP/(TP+FN)
  TNR<-TN/(TN+FP)
  PPV<-TP/(TP+FP)
  NPV<-TN/(TN+FN)

#Out of Sample Prediction
  pred.logit.test<-predict.glm(logistic.final,newdata=test)
  pred.expit.test<- exp(pred.logit.test)/(1+exp(pred.logit.test))
  
  a<-which(test$Class==1)
  b<-which(test$Class==0)
  
  TP.test<-length(which(pred.expit.test[a]>0.5))
  FN.test<-length(which(pred.expit.test[a]<=0.5))
  TN.test<-length(which(pred.expit.test[b]<0.5))
  FP.test<-length(which(pred.expit.test[b]>=0.5))
  Recall.test<-TP.test/(TP.test+FN.test)
  TNR.test<-TN.test/(TN.test+FP.test)
  PPV.test<-TP.test/(TP.test+FP.test)
  NPV.test<-TN.test/(TN.test+FN.test)
```

The training set and test set accuracy metrics show that the final logistic model is able to accurately and reliably label legitimate credit card transactions. This is demonstrated by TNR and NPV values exceeding 0.99. 

It does not have the same degree of accuracy for fraudulent transactions with Recall values of only 0.88. In addition, the PPV on the test set suggests that any transaction labeled as fraudulent is correct only 59% of the time. This essentially disqualifies the final logistic model from having any in-field applications.

```{r}
##Summary
logit.DT<-data.table(data_set=c('train','test'),
                     recall = c(Recall,Recall.test),
                     true.negative.rate=c(TNR,TNR.test),
                     pos.pred.value=c(PPV,PPV.test),
                     neg.pred.value=c(NPV,NPV.test))

logit.DT
```

#Machine Learning Classifiers

The predictive performance of 1 gradient boosting classifier and 2 support vector machine classifiers are investigated. Before training these types of models, the Class variables in the sub-training datasets need be converted into factor variables.

```{r}
for(i in 1:10){
  invisible(train.sub[[i]][,Class:=as.factor(Class)])
}
```

##Gradient Boosting with Trees: Cross-Validation 

GBM classifiers use weighted decision trees as the weak learners. The tree depth needed to get high accuracy is typically less than 3. As with the logit regression model, multiple GBM classifiers are created using the sub-training data sets. Each of the 10 gradient boosting models is able to classify fraudulent transactions on the entire training set moderately well with the least accurate model having a fraud sensitivity of 0.878. 


```{r}
gbm.fit<-list()

  set.seed(1234)
  for(i in 1:10){
      gbm.fit[[i]]<-train(data = train.sub[[i]], Class~.-Time,method='gbm',verbose=FALSE)
  }


#Calculate Recall for All GBM Models on Entire Training Set
    gbm.recall=c()
    gbm.specificity=c()
  
    a<-which(train$Class=='1')
    b<-which(train$Class=='0')
  
    for(i in 1:10){
          gbm.recall[i]<-length(which(predict(gbm.fit[[i]],newdata =train)[a]=='1'))/length(a)
          gbm.specificity[i]<-length(which(predict(gbm.fit[[i]],newdata =train)[b]=='0'))/length(b)
    }
      data.table(Model=c(1:10),recall=gbm.recall,spec=gbm.specificity)
```

The final gbm model will be a composite of all 10 classifiers. It labels transactions as fraudulent if a minimum number of the predictors classify it as fraudulent. To determine what that threshold should be, the true positive rate(TPR) was calculated for predictor thresholds varying from 1-10. The results show that the final model is able to label a fraudulent training set transaction best when a minimum of 1 gbm model classifies it as fraudulent.

It should be noted that this procedure of classifying fraud based on a minimum number of votes appears to be a robust approach for all thresholds. As shown in the ROC curve for the training data, all the data points are dramatically above the 45 degree line with a minimum TPR of 0.948 (threshold = 10). Based on the plotted values and the fact that TPR=1 & FPR=1 for thresholds to the right of the last point (labels everything as fraud), the **AUC for the entire final model is greater than 0.9** making it an extremely good classifier of fraudulent transactions.

```{r}
#Final GBM Model - Validation on Training Set
    v1<-predict(gbm.fit[[1]],newdata=train)
    v2<-predict(gbm.fit[[2]],newdata=train)
    v3<-predict(gbm.fit[[3]],newdata=train)
    v4<-predict(gbm.fit[[4]],newdata=train)
    v5<-predict(gbm.fit[[5]],newdata=train)
    v6<-predict(gbm.fit[[6]],newdata=train)
    v7<-predict(gbm.fit[[7]],newdata=train)
    v8<-predict(gbm.fit[[8]],newdata=train)
    v9<-predict(gbm.fit[[9]],newdata=train)
    v10<-predict(gbm.fit[[10]],newdata=train)
    
    v.agg<-as.numeric(v1)+as.numeric(v2)+as.numeric(v3)+as.numeric(v4)+as.numeric(v5)+
      as.numeric(v6)+as.numeric(v7)+as.numeric(v8)+as.numeric(v9)+as.numeric(v10)-10
    
    final.gbm<-data.table(v.agg, truth=train$Class)
    
    gbm.TPR<-c()
    gbm.FPR<-c()

    a<-which(train$Class=='1')
    b<-which(train$Class=='0')

    for(i in 1:10){
        final.gbm[,prediction:=as.factor(ifelse(v.agg>=i,'1','0'))]
        gbm.TPR[i]<-length(which(final.gbm$prediction[a]=='1'))/length(a)
        gbm.FPR[i]<-length(which(final.gbm$prediction[b]=='1'))/length(b)
    }
    
    ggplot(data=data.table(threshold=c(1:10),TPR=gbm.TPR,FPR=gbm.FPR),aes(x=threshold,y=TPR))+
        geom_point(colour='blue')+geom_line()+
        ylim(0.8,1)+xlim(1,10)+xlab('Threshold')+ylab('TPR')+ggtitle('Threshold Effect on GBM TPR')
    
    ggplot(data=data.table(threshold=as.factor(c(1:10)),TPR=gbm.TPR,FPR=gbm.FPR),aes(x=FPR,y=TPR,colour=threshold))+
        geom_point()+geom_line()+
        ylim(0,1)+xlim(0,1)+
        geom_line(data=data.table(x=c(0,1),y=c(0,1)),aes(x=x,y=y),colour='black')+
        xlab('FPR')+ylab('TPR')+ggtitle('Partial ROC Curve for Final GBM')
    
    
```
    
The final composite gradient boosting model is slightly better at correctly labeling fraud than the traditional logistic regression with a test set recall of 0.903, which is corresponds to a 3% improvement. However, it has a very poor test set PPV of 0.104 meaning that most of the predictions of fraudulent transactions would be false alarms. 

It is likely that the overly conservative predictions are a result of the rebalancing of the training data to an 8:1 legitimate-to-fraud ratio. A good next step to improve the composite gbm model would be to run a rebalancing study to see (1) how it affects the PPV and (2) how much the ratio can be increased before the recall decreases below an acceptable threshold.
```{r}
#Train
    final.gbm[,prediction:=as.factor(ifelse(v.agg>0,'1','0'))]

    a<-which(train$Class==1)
    b<-which(train$Class==0)
    
    TP.train<-length(which(final.gbm$prediction[a]=='1'))
    FN.train<-length(which(final.gbm$prediction[a]=='0'))
    TN.train<-length(which(final.gbm$prediction[b]=='0'))
    FP.train<-length(which(final.gbm$prediction[b]=='1'))
    Recall.train<-TP.train/(TP.train+FN.train)
    TNR.train<-TN.train/(TN.train+FP.train)
    PPV.train<-TP.train/(TP.train+FP.train)
    NPV.train<-TN.train/(TN.train+FN.train)

#Final GBM Model (Test Set)
    v1<-predict(gbm.fit[[1]],newdata=test)
    v2<-predict(gbm.fit[[2]],newdata=test)
    v3<-predict(gbm.fit[[3]],newdata=test)
    v4<-predict(gbm.fit[[4]],newdata=test)
    v5<-predict(gbm.fit[[5]],newdata=test)
    v6<-predict(gbm.fit[[6]],newdata=test)
    v7<-predict(gbm.fit[[7]],newdata=test)
    v8<-predict(gbm.fit[[8]],newdata=test)
    v9<-predict(gbm.fit[[9]],newdata=test)
    v10<-predict(gbm.fit[[10]],newdata=test)
    
    v.agg<-as.numeric(v1)+as.numeric(v2)+as.numeric(v3)+as.numeric(v4)+as.numeric(v5)+
      as.numeric(v6)+as.numeric(v7)+as.numeric(v8)+as.numeric(v9)+as.numeric(v10)-10    
    
    final.gbm.test<-data.table(v.agg, truth=as.factor(test$Class))
    final.gbm.test[,prediction:=as.factor(ifelse(v.agg>0,'1','0'))]
    
    a<-which(test$Class=='1')
    b<-which(test$Class=='0')
    
    TP.test<-length(which(final.gbm.test$prediction[a]=='1'))
    FN.test<-length(which(final.gbm.test$prediction[a]=='0'))
    TN.test<-length(which(final.gbm.test$prediction[b]=='0'))
    FP.test<-length(which(final.gbm.test$prediction[b]=='1'))
    Recall.test<-TP.test/(TP.test+FN.test)
    TNR.test<-TN.test/(TN.test+FP.test)
    PPV.test<-TP.test/(TP.test+FP.test)
    NPV.test<-TN.test/(TN.test+FN.test)

gbm.DT<-data.table(data_set=c('train','test'),
                     recall = c(Recall.train,Recall.test),
                     true.negative.rate=c(TNR.train,TNR.test),
                     pos.pred.value=c(PPV.train,PPV.test),
                     neg.pred.value=c(NPV.train,NPV.test))

gbm.DT
```

##Support Vector Machine (Standard 2-Class Classification)

Support vector machines are classically used to distinguish between different groups by defining a separating hyperplane based on the predictor datapoints. If there exists   

```{r}
#Use nonfraud-to-fraud ratio of 4:1 

set.seed(1234)
    svm1.fit<-list()
    for(i in 1:10){
        svm1.fit[[i]]<-train(data = train.sub[[i]], as.factor(Class)~.-Time,
                        method='svmRadial',
                        verbose=FALSE)
        #print(paste0(c('SVM Model: ',i),collapse = ''))
        svm1.fit[[i]]
    }
    
#Final SVM Model (Training Set)
    v1<-predict(svm1.fit[[1]],newdata=train)
    v2<-predict(svm1.fit[[2]],newdata=train)
    v3<-predict(svm1.fit[[3]],newdata=train)
    v4<-predict(svm1.fit[[4]],newdata=train)
    v5<-predict(svm1.fit[[5]],newdata=train)
    v6<-predict(svm1.fit[[6]],newdata=train)
    v7<-predict(svm1.fit[[7]],newdata=train)
    v8<-predict(svm1.fit[[8]],newdata=train)
    v9<-predict(svm1.fit[[9]],newdata=train)
    v10<-predict(svm1.fit[[10]],newdata=train)
    
    v.agg<-as.numeric(v1)+as.numeric(v2)+as.numeric(v3)+as.numeric(v4)+as.numeric(v5)+
      as.numeric(v6)+as.numeric(v7)+as.numeric(v8)+as.numeric(v9)+as.numeric(v10)-10    
    
    final.svm1.train<-data.table(v.agg, truth=as.factor(train$Class))
    invisible(final.svm1.train[,prediction:=as.factor(ifelse(v.agg>1,'1','0'))])
    
    a<-which(train$Class==1)
    b<-which(train$Class==0)
    
    TP.train<-length(which(final.svm1.train[a]$prediction=='1'))
    FP.train<-length(which(final.svm1.train[a]$prediction=='0'))
    TN.train<-length(which(final.svm1.train[b]$prediction=='0'))
    FN.train<-length(which(final.svm1.train[b]$prediction=='1'))
    Recall.train<-TP.train/(TP.train+FN.train)
    TNR.train<-TN.train/(TN.train+FP.train)
    PPV.train<-TP.train/(TP.train+FP.train)
    NPV.train<-TN.train/(TN.train+FN.train)
    
#Final SVM Model (Test Set)
    v1<-predict(svm1.fit[[1]],newdata=test)
    v2<-predict(svm1.fit[[2]],newdata=test)
    v3<-predict(svm1.fit[[3]],newdata=test)
    v4<-predict(svm1.fit[[4]],newdata=test)
    v5<-predict(svm1.fit[[5]],newdata=test)
    v6<-predict(svm1.fit[[6]],newdata=test)
    v7<-predict(svm1.fit[[7]],newdata=test)
    v8<-predict(svm1.fit[[8]],newdata=test)
    v9<-predict(svm1.fit[[9]],newdata=test)
    v10<-predict(svm1.fit[[10]],newdata=test)
    
    v.agg<-as.numeric(v1)+as.numeric(v2)+as.numeric(v3)+as.numeric(v4)+as.numeric(v5)+
      as.numeric(v6)+as.numeric(v7)+as.numeric(v8)+as.numeric(v9)+as.numeric(v10)-10    
    
    final.svm1.test<-data.table(v.agg, truth=as.factor(test$Class))
    invisible(final.svm1.test[,prediction:=as.factor(ifelse(v.agg>1,'1','0'))])
    
    a<-which(test$Class==1)
    b<-which(test$Class==0)
    
    TP.test<-length(which(final.svm1.test[a]$prediction=='1'))
    FP.test<-length(which(final.svm1.test[a]$prediction=='0'))
    TN.test<-length(which(final.svm1.test[b]$prediction=='0'))
    FN.test<-length(which(final.svm1.test[b]$prediction=='1'))
    Recall.test<-TP.test/(TP.test+FN.test)
    TNR.test<-TN.test/(TN.test+FP.test)
    PPV.test<-TP.test/(TP.test+FP.test)
    NPV.test<-TN.test/(TN.test+FN.test)

svm1.DT<-data.table(data_set=c('train','test'),
                     recall = c(Recall.train,Recall.test),
                     true.negative.rate=c(TNR.train,TNR.test),
                     pos.pred.value=c(PPV.train,PPV.test),
                     neg.pred.value=c(NPV.train,NPV.test))

svm1.DT
```

#Support Vector Machine (Novelty Detection via Single Class SVM)

An alternative to multi-class classification with SVM is 1-class classification, or novelty detection. It's a little bit odd of a classification method because it constructs a 'separating' hyperplane based only on the data from 1 class. If all the information that uniquely describes the legitimate data is contained in the dataset, a descrimination algorithm can be created without having to make any adjustments to account imbalance between the legitimate and fraudulent data in the **primary training set** or the **test set**.  

```{r}
svm2.data<-train[nonfraud]
svm2.data[,Class:=as.factor(Class)]

set.seed(1234)
svm2.fit<-list()
for(i in 1:10){
    svm2.fit[[i]]<- ksvm(Class~.-Time,data=svm2.data,nu=0.1/i,type='one-svc')
    print(paste0(c('Model ',i),collapse = ''))
    print(svm2.fit[[i]])
}

svm2.validate<-list()
TP<-c()
FN<-c()
sens<-c()
temp_data<-train[fraud,]
for(i in 1:10){
    svm2.validate[[i]]<-predict(svm2.fit[[i]],newdata=temp_data,type='response')
    TP[i]<-length(which(svm2.validate[[i]]==FALSE))
    FN[i]<-length(which(svm2.validate[[i]]==TRUE))
    sens[i]<-TP[i]/(TP[i]+FN[i])
}

print('Model With Best Training Set Fraud Detection')
best_novelty<-which(sens==max(sens))
svm2.fit[best_novelty]

print('Best Model Performance on Test Set Fraud Data')
a<- which(test$Class=='1')
b<- which(test$Class=='0')

svm2.test<-predict(svm2.fit[[best_novelty]],newdata=test[a,],type='response')
TP.test<-length(which(svm2.test==FALSE))
FN.test<-length(which(svm2.test==TRUE))
sens.test<-TP.test/(TP.test+FN.test)
sens.test
```






